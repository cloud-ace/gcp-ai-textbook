{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai_shoseki_3_3_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrhmFCtyDewE"
      },
      "source": [
        "# 必要なモジュールをインストール\n",
        "!pip install lxml cssselect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uUyTAal_5_B"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import codecs\n",
        "import requests\n",
        "import urllib\n",
        "from urllib.request import urlopen\n",
        "import lxml.html\n",
        "import zipfile\n",
        " \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "from google.colab import auth\n",
        " \n",
        "ROOT_DIR = '.' + os.path.sep + 'aozora' + os.path.sep\n",
        "ZIP_DIR = ROOT_DIR + 'zip' + os.path.sep\n",
        "TEXT_SJIS_DIR = ROOT_DIR + 'text_sjis' + os.path.sep\n",
        "TEXT_UTF8_DIR = ROOT_DIR + 'text_utf8' + os.path.sep\n",
        "GS_DIR = 'gs://バケット名/aozora_dataset/'\n",
        "DATA_LIST_FILENAME = 'data_list.csv'\n",
        " \n",
        "base_url = 'http://www.aozora.gr.jp/'\n",
        " \n",
        "author_dict = {\n",
        "    'author' : [\n",
        "                'NatsumeSoseki', \n",
        "                'DazaiOsamu', \n",
        "                'MiyazawaKenji',\n",
        "                'AkutagawaRyunosuke',\n",
        "                'YumenoKyusaku',\n",
        "                'FukuzawaYukichi'\n",
        "                ], \n",
        "    'url' : [\n",
        "             'http://www.aozora.gr.jp/index_pages/person148.html#sakuhin_list_1',\n",
        "             'https://www.aozora.gr.jp/index_pages/person35.html#sakuhin_list_1',\n",
        "             'https://www.aozora.gr.jp/index_pages/person81.html#sakuhin_list_1',\n",
        "             'https://www.aozora.gr.jp/index_pages/person879.html#sakuhin_list_1',\n",
        "             'https://www.aozora.gr.jp/index_pages/person96.html#sakuhin_list_1',\n",
        "             'https://www.aozora.gr.jp/index_pages/person296.html#sakuhin_list_1'\n",
        "            ],\n",
        "    'download_num' : [\n",
        "                      12, \n",
        "                      23, \n",
        "                      27,\n",
        "                      20,\n",
        "                      8,\n",
        "                      35\n",
        "                      ],\n",
        "    'exclusion_list' : [\n",
        "        ['こころ', '永日小品'],\n",
        "        ['走れメロス'],\n",
        "        ['銀河鉄道の夜', '〔青びかる天弧のはてに〕', '青柳教諭を送る', '〔あくたうかべる朝の水〕'],\n",
        "        ['羅生門'],\n",
        "        ['ドグラ・マグラ'],\n",
        "        ['学問のすすめ']\n",
        "    ]\n",
        "}\n",
        "author_df = pd.DataFrame(author_dict)\n",
        " \n",
        " \n",
        "def download_zip(author_df):\n",
        "    for index, row in author_df.iterrows():\n",
        "        zip_dict = {}\n",
        " \n",
        "        url = row['url']\n",
        "        resp_root = requests.get(url)\n",
        "        html_root = lxml.html.fromstring(resp_root.content)\n",
        "        html_root.make_links_absolute(resp_root.url)\n",
        " \n",
        "        print('author : {}'.format(row['author']))\n",
        "        print('url : {}'.format(url))\n",
        " \n",
        "        download_count = 0\n",
        "        for a in html_root.cssselect('a'):\n",
        "            if -1 < row['download_num'] and row['download_num'] <= download_count:\n",
        "                break\n",
        " \n",
        "            link = a.get('href')\n",
        "            if link is not None and 'cards' in link:\n",
        "                resp_card = requests.get(link)\n",
        "                html_card = lxml.html.fromstring(resp_card.content)\n",
        "                html_card.make_links_absolute(resp_card.url)\n",
        "                link_zip_list = html_card.xpath('//a[contains(@href, \".zip\")]')\n",
        "                if 0 < len(link_zip_list):\n",
        "                    link_zip = link_zip_list[0]\n",
        "                    zip_filename = link_zip.text.split('.')[-2]\n",
        "                    zip_link = link_zip.get('href')\n",
        "                    print('[{}],{},{}'. format(a.text, zip_filename, zip_link))\n",
        " \n",
        "                    # 使用しないファイルをスキップ\n",
        "                    if a.text in row['exclusion_list']:\n",
        "                        print('    {} is excluded'.format(a.text))\n",
        "                        continue\n",
        " \n",
        "                    zip_dict[zip_filename] = zip_link\n",
        "                    download_count += 1\n",
        " \n",
        "        download_path = ZIP_DIR + row['author'] + os.path.sep\n",
        "        if not os.path.exists(download_path):\n",
        "            os.mkdir(download_path)\n",
        "        for i, key in enumerate(zip_dict):\n",
        "            print('download[{}/{}]...    {} : {}'.format(i+1, len(zip_dict), key, zip_dict[key]))\n",
        "            urllib.request.urlretrieve(zip_dict[key], download_path + os.path.basename(zip_dict[key]))\n",
        " \n",
        " \n",
        "def extract_zip(author_df):\n",
        "    for index, row in author_df.iterrows():\n",
        "        author_zip_dir = ZIP_DIR + row['author'] + os.path.sep\n",
        "        author_text_sjis_dir = TEXT_SJIS_DIR + row['author'] + os.path.sep\n",
        "        if not os.path.exists(author_text_sjis_dir):\n",
        "            os.mkdir(author_text_sjis_dir)\n",
        " \n",
        "        files = os.listdir(author_zip_dir)\n",
        "        for file in files:\n",
        "            root, ext = os.path.splitext(file)\n",
        "            if ext == '.zip':\n",
        "                with zipfile.ZipFile(author_zip_dir + file, 'r') as zf:\n",
        "                    zf.extractall(path=author_text_sjis_dir)\n",
        "                    print('extract {} to {}'.format(author_zip_dir + file, author_text_sjis_dir))\n",
        " \n",
        " \n",
        "def prepare_data(author_df):\n",
        "    re_ruby = re.compile('\\《.+?\\》')\n",
        "    re_note = re.compile('\\［＃.+?\\］')\n",
        "    for index, row in author_df.iterrows():\n",
        "        author_text_sjis_dir = TEXT_SJIS_DIR + row['author'] + os.path.sep\n",
        " \n",
        "        files = os.listdir(author_text_sjis_dir)\n",
        "        for file in files:\n",
        "            root, ext = os.path.splitext(file)\n",
        "            if ext == '.txt':\n",
        " \n",
        "                sjis_filename = author_text_sjis_dir + file\n",
        "                fsjis = codecs.open(sjis_filename, 'r', 'shift_jis')\n",
        "                blank_line_count = 0\n",
        "                paragraph_index = 0\n",
        "                is_header = False\n",
        "                for i, line in enumerate(fsjis):\n",
        "                    line = line.strip()\n",
        " \n",
        "                    # 空行はスキップ\n",
        "                    if len(line) == 0:\n",
        "                        blank_line_count += 1\n",
        "                        continue\n",
        " \n",
        "                    # ヘッダはスキップ\n",
        "                    if line.startswith('--------------------'):\n",
        "                        is_header = not is_header\n",
        "                        continue\n",
        "                    if is_header:\n",
        "                        continue\n",
        " \n",
        "                    # フッタはスキップ\n",
        "                    if 3 <= blank_line_count:\n",
        "                        if line.startswith('底本：'):\n",
        "                            break\n",
        "                    blank_line_count = 0\n",
        " \n",
        "                    # ルビ、注釈は削除\n",
        "                    edited_line = re_note.sub('', re_ruby.sub('', line.replace('\\r', '')))\n",
        "                    if len(edited_line) == 0:\n",
        "                        continue\n",
        " \n",
        "                    # 段落ごとに utf8 のテキストファイルを作成\n",
        "                    paragraph_index += 1\n",
        "                    utf8_filename = '{}{}_{}_{}.txt'.format(TEXT_UTF8_DIR, row['author'], root, str(paragraph_index).zfill(5))\n",
        "                    futf8 = codecs.open(utf8_filename, 'w', 'utf-8')\n",
        "                    futf8.write(''.join(edited_line))\n",
        "                    futf8.close()\n",
        " \n",
        "                print('{} : {}'.format(sjis_filename, paragraph_index))\n",
        "                fsjis.close()\n",
        " \n",
        " \n",
        "def create_data_list(filename):\n",
        "    file_list = []\n",
        "    label_list = []\n",
        " \n",
        "    text_files = os.listdir(TEXT_UTF8_DIR)\n",
        "    for file in text_files:\n",
        "        root, ext = os.path.splitext(file)\n",
        "        file_list.append(GS_DIR + 'text/' + file)\n",
        "        label_list.append(root.split('_')[0])\n",
        " \n",
        "    data_dict = {'file_url': file_list, 'label': label_list}\n",
        "    data_df = pd.DataFrame(data_dict)\n",
        "    print('data_df : {}'.format(data_df))\n",
        "    data_df.to_csv(ROOT_DIR + filename, index=False, header=False)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLKflsHuX7DH"
      },
      "source": [
        "# 必要なディレクトリを作成\n",
        "os.makedirs(ZIP_DIR)\n",
        "os.makedirs(TEXT_SJIS_DIR)\n",
        "os.makedirs(TEXT_UTF8_DIR)\n",
        " \n",
        "print()\n",
        "print('# 青空文庫からテキストデータの zip ファイルをダウンロード ####################')\n",
        "print()\n",
        "download_zip(author_df)\n",
        " \n",
        "print()\n",
        "print('# ダウンロードした zip ファイルを解凍 ##################################')\n",
        "print()\n",
        "extract_zip(author_df)\n",
        " \n",
        "print()\n",
        "print('# 解凍したテキストデータを前処理 ####################################')\n",
        "print()\n",
        "prepare_data(author_df)\n",
        " \n",
        "print()\n",
        "print('# 学習で使用するデータリストファイルを作成 #############################')\n",
        "print()\n",
        "create_data_list(DATA_LIST_FILENAME)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OL9smXEYNbJ"
      },
      "source": [
        "# auth.authenticate_user() で GCP への認証を行う\n",
        "auth.authenticate_user()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7oV7D85Yc_e"
      },
      "source": [
        "# テキストデータファイルを GCS へアップロード\n",
        "!gsutil -m cp aozora/text_utf8/* gs://バケット名/aozora_dataset/text/\n",
        " \n",
        "# データリストファイルを GCS へアップロード\n",
        "!gsutil -m cp aozora/data_list.csv gs://バケット名/aozora_dataset/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q8EC79F5hZg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
